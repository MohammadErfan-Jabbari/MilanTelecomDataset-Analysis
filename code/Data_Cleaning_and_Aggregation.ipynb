{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Aggregation Pipeline\n",
    "\n",
    "## Overview\n",
    "This Jupyter notebook contains the pipeline designed for cleaning, aggregating, and resampling our datasets of telecommunications data from city of Milan. \n",
    "This datasets contains information about SMS, calls, and internet traffic across different geographical squares over time. \n",
    "The objective is to prepare the data for further analysis by selecting relevant columns, handling missing values, aggregating data by time intervals, and resampling the traffic data to a consistent frequency.\n",
    "\n",
    "## Dataset Structure\n",
    "The datasets to be processed are expected to have the following columns:\n",
    "- `square_id`: The ID of the geographical square.\n",
    "- `time_interval`: The timestamp, indicating the interval of data collection.\n",
    "- `country_code`: The country code for the data entry.\n",
    "- `sms_in`, `sms_out`: The number of incoming and outgoing SMS messages.\n",
    "- `call_in`, `call_out`: The number of incoming and outgoing calls.\n",
    "- `internet_traffic`: The amount of internet traffic.\n",
    "\n",
    "Files are in text format, with data entries separated by tabs, and are named according to the date of data collection (e.g., `sms-call-internet-mi-2013-11-01.txt`).\n",
    "\n",
    "## Prerequisites\n",
    "Before running this notebook, ensure that you have the following prerequisites:\n",
    "1. **Python Environment**: A Python environment with Python 3.6 or later.\n",
    "2. **Pandas Library**: The Pandas library installed for data manipulation and analysis. Install it using `pip install pandas` if not already installed.\n",
    "3. **Dataset**: Ensure that your dataset files are placed in a designated folder. This code assumes that your raw datasets are stored in `./txt_dataset/` and the cleaned datasets will be saved to `./cleaned_dataset_30/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necassary imports\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions\n",
    "Below are the modular functions used in this notebook for reading and preprocessing data, aggregating data, resampling data, cleaning individual files, and processing multiple files in a directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_preprocess(file_path, selected_columns):\n",
    "    \"\"\"\n",
    "    Reads and preprocesses the data from a given file.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path: The path to the file to be read.\n",
    "    - selected_columns: The columns to select from the dataset.\n",
    "    \n",
    "    Returns:\n",
    "    A DataFrame with the selected columns, filled missing values, and processed data.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path, sep='\\t', names=['square_id', 'time_interval', 'country_code', 'sms_in', 'sms_out', 'call_in', 'call_out', 'internet_traffic'])\n",
    "    df.fillna(0, inplace=True)\n",
    "    return df[selected_columns]\n",
    "\n",
    "def aggregate_data(df):\n",
    "    \"\"\"\n",
    "    Aggregates data by square_id and time_interval, sums the values, and resets the index.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: The DataFrame to be aggregated.\n",
    "    \n",
    "    Returns:\n",
    "    An aggregated DataFrame with time_interval converted to datetime and set as index.\n",
    "    \"\"\"\n",
    "    df = df.groupby(['square_id', 'time_interval']).sum().reset_index()\n",
    "    df['time_interval'] = pd.to_datetime(df['time_interval'], unit=\"ms\")\n",
    "    df.set_index('time_interval', inplace=True)\n",
    "    return df\n",
    "\n",
    "def resample_data(df, frequency='H'):\n",
    "    \"\"\"\n",
    "    Resamples the DataFrame to the specified frequency, summing the internet traffic.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: The DataFrame to be resampled.\n",
    "    - frequency: The frequency for resampling, default is 'H' (hourly).\n",
    "    \n",
    "    Returns:\n",
    "    A resampled DataFrame with internet traffic summed up.\n",
    "    \"\"\"\n",
    "    return df.groupby('square_id').resample(frequency)['internet_traffic'].agg('sum').loc[lambda x: x>0].reset_index()\n",
    "\n",
    "def clean_data(file_path, selected_columns, destination_path):\n",
    "    \"\"\"\n",
    "    Cleans the data by reading, preprocessing, aggregating, and resampling,\n",
    "    then saves the cleaned data to a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path: The path to the source file, excluding the extension.\n",
    "    - selected_columns: The columns to be selected for processing.\n",
    "    - destination_path: The path to save the cleaned data, excluding the extension.\n",
    "    \"\"\"\n",
    "    df = read_and_preprocess(file_path + '.txt', selected_columns)\n",
    "    df = aggregate_data(df)\n",
    "    df = resample_data(df)\n",
    "    df.to_csv(destination_path + '.csv', index=False)\n",
    "\n",
    "def process_files(source_folder, destination_folder, selected_columns):\n",
    "    \"\"\"\n",
    "    Processes all files in the source folder, cleans the data, and saves it to the destination folder.\n",
    "    \n",
    "    Parameters:\n",
    "    - source_folder: The path to the folder containing source files.\n",
    "    - destination_folder: The path to the folder where cleaned files will be saved.\n",
    "    - selected_columns: The columns to be selected for processing.\n",
    "    \"\"\"\n",
    "    file_list = [file for file in os.listdir(source_folder) if file.endswith('.txt')]\n",
    "    for file_name in file_list:\n",
    "        base_name = os.path.splitext(file_name)[0]\n",
    "        clean_data(os.path.join(source_folder, base_name), selected_columns, os.path.join(destination_folder, 'cleaned-' + base_name))\n",
    "        print(f\"{destination_folder}cleaned-{base_name}.csv finished \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution\n",
    "This section executes the data cleaning pipeline on all files in the specified source folder, demonstrating the pipeline's application to real-world datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder paths and selected columns for cleaning the data\n",
    "txt_dataset_folder = './txt_dataset/'\n",
    "cleaned_dataset_folder = './cleaned_dataset/'\n",
    "# cleaned_dataset_folder_30minutes_interval = './cleaned_dataset/'\n",
    "selected_columns = ['square_id', 'time_interval', 'internet_traffic']\n",
    "\n",
    "# Start processing files from the source folder and save cleaned data to the destination folder\n",
    "process_files(txt_dataset_folder, cleaned_dataset_folder, selected_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This notebook provides a detailed walkthrough of the data cleaning and aggregation pipeline tailored for telecommunications data. By leveraging the modular functions defined and executed in this notebook, we can efficiently process raw datasets to extract meaningful insights and prepare the data for further analysis.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
